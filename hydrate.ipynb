{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Developed by Humza, Afnan, Saad, Ahsan, Nabeel\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from twarc import Twarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code to take a sample of tweets for hydration\n",
    "\n",
    "list_dir = ['2020-09_og']\n",
    "final_ids = dict()\n",
    "\n",
    "for direct in list_dir:\n",
    "    n_files = list()\n",
    "    path_dir = os.getcwd() + \"\\\\\" + direct\n",
    "    og_files = glob.glob(path_dir + \"/*.txt\")\n",
    "    for file in og_files:\n",
    "        ids = pd.read_csv(file,header=None)\n",
    "        sample = ids.iloc[:,0].sample(n=40000,random_state=1,replace=False)\n",
    "        sample = list(sample)\n",
    "        n_files.append(sample)\n",
    "    n_files = np.concatenate(n_files)\n",
    "    final_ids[direct] = n_files\n",
    "\n",
    "for direct in final_ids.keys():\n",
    "    new_dir = direct.split(\"_\")[0] \n",
    "    os.mkdir(new_dir)\n",
    "    path_upd = os.getcwd() + \"\\\\\" + new_dir\n",
    "    if '-09' in direct:\n",
    "        np.savetxt((path_upd + '\\\\pre_pres_deb1.txt'),final_ids[direct],fmt='%d')\n",
    "    elif '-10' in direct:\n",
    "        np.savetxt((path_upd + '\\\\post_pres_deb1.txt'),final_ids[direct],fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hydrating BLM_test\\test.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████                                                                  | 1434/10000 [00:54<05:23, 26.50it/s]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# This script will walk through all the tweet id files and\n",
    "# hydrate them with twarc. The line oriented JSON files will\n",
    "# be placed right next to each tweet id file.\n",
    "#\n",
    "# Note: you will need to install twarc, tqdm, and run twarc configure\n",
    "# from the command line to tell it your Twitter API keys.\n",
    "#\n",
    "# Special thanks to Github users edsu and SamSamhuns for contributing to this file. This file was repurposed from our other\n",
    "# data repository on COVID-19 related tweets : https://github.com/echen102/COVID-19-TweetIDs\n",
    "#\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from twarc import Twarc\n",
    "from pathlib import Path\n",
    "\n",
    "twarc = Twarc()\n",
    "data_dirs = ['BLM_test'] # can give multiple directories if you have made tweet id files for separate months. follow the naming scheme i.e. YYYY-MM\n",
    "\n",
    "def main():\n",
    "    for data_dir in data_dirs:\n",
    "        for path in Path(data_dir).iterdir():\n",
    "            if path.name.endswith('.txt'):\n",
    "                hydrate(path)\n",
    "\n",
    "\n",
    "def _reader_generator(reader):\n",
    "    b = reader(1024 * 1024)\n",
    "    while b:\n",
    "        yield b\n",
    "        b = reader(1024 * 1024)\n",
    "\n",
    "\n",
    "def raw_newline_count(fname):\n",
    "    \"\"\"\n",
    "    Counts number of lines in file\n",
    "    \"\"\"\n",
    "    f = open(fname, 'rb')\n",
    "    f_gen = _reader_generator(f.raw.read)\n",
    "    return sum(buf.count(b'\\n') for buf in f_gen)\n",
    "\n",
    "\n",
    "def hydrate(id_file):\n",
    "    print('hydrating {}'.format(id_file))\n",
    "\n",
    "    gzip_path = id_file.with_suffix('.jsonl.gz')\n",
    "    if gzip_path.is_file():\n",
    "        print('skipping json file already exists: {}'.format(gzip_path))\n",
    "        return\n",
    "\n",
    "    num_ids = raw_newline_count(id_file)\n",
    "\n",
    "    with gzip.open(gzip_path, 'w') as output:\n",
    "        with tqdm(total=num_ids) as pbar:\n",
    "            for tweet in twarc.hydrate(id_file.open()):\n",
    "                output.write(json.dumps(tweet).encode('utf8') + b\"\\n\")\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this after running the second cell which hydrated the ids. Use the directory named based on folder name where you stored id file. \n",
    "# Format for directory is YYYY-MM\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "data_dirs = ['2020-09']\n",
    "hyd_tweets = dict()\n",
    "\n",
    "for directory in data_dirs:\n",
    "    path = os.getcwd() + f'\\\\{directory}\\\\'\n",
    "    all_files = glob.glob(path + \"/*.gz\")\n",
    "    li = []\n",
    "    for filename in all_files:\n",
    "        df = pd.read_json(filename,compression='infer',lines=True)\n",
    "        li.append(df)\n",
    "    data = pd.concat(li, axis=0, ignore_index=True)\n",
    "    hyd_tweets[directory] = data\n",
    "    data.to_csv(path + directory + '.csv')\n",
    "    \n",
    "# finally files created for exploration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
