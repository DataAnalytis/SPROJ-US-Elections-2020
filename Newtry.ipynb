{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/10001 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hydrating C:\\Users\\Afnan Anwar\\Desktop\\SPROJ\\Cleaning\\target.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|█████████████████████████████████████████████████████████████████▊           | 8548/10001 [02:58<00:30, 47.93it/s]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# This script will walk through all the tweet id files and\n",
    "# hydrate them with twarc. The line oriented JSON files will\n",
    "# be placed right next to each tweet id file.\n",
    "#\n",
    "# Note: you will need to install twarc, tqdm, and run twarc configure\n",
    "# from the command line to tell it your Twitter API keys.\n",
    "#\n",
    "# Special thanks to Github users edsu and SamSamhuns for contributing to this file. This file was repurposed from our other\n",
    "# data repository on COVID-19 related tweets : https://github.com/echen102/COVID-19-TweetIDs\n",
    "#\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from twarc import Twarc\n",
    "from pathlib import Path\n",
    "\n",
    "twarc = Twarc()\n",
    "data_dirs = ['C:\\\\Users\\\\Afnan Anwar\\\\Desktop\\\\SPROJ\\\\Cleaning\\\\']\n",
    "\n",
    "\n",
    "def main():\n",
    "    for data_dir in data_dirs:\n",
    "       for path in Path(data_dir).iterdir():\n",
    "            if path.name.endswith('.txt'):\n",
    "                hydrate(path)\n",
    "\n",
    "\n",
    "def _reader_generator(reader):\n",
    "    b = reader(1024 * 1024)\n",
    "    while b:\n",
    "        yield b\n",
    "        b = reader(1024 * 1024)\n",
    "\n",
    "\n",
    "def raw_newline_count(fname):\n",
    "    \"\"\"\n",
    "    Counts number of lines in file\n",
    "    \"\"\"\n",
    "    f = open(fname, 'rb')\n",
    "    f_gen = _reader_generator(f.raw.read)\n",
    "    return sum(buf.count(b'\\n') for buf in f_gen)\n",
    "\n",
    "\n",
    "def hydrate(id_file):\n",
    "    print('hydrating {}'.format(id_file))\n",
    "\n",
    "    gzip_path = id_file.with_suffix('.jsonl.gz')\n",
    "    if gzip_path.is_file():\n",
    "        print('skipping json file already exists: {}'.format(gzip_path))\n",
    "        return\n",
    "\n",
    "    num_ids = raw_newline_count(id_file)\n",
    "\n",
    "    with gzip.open(gzip_path, 'w') as output:\n",
    "        with tqdm(total=num_ids) as pbar:\n",
    "            for tweet in twarc.hydrate(id_file.open()):\n",
    "                output.write(json.dumps(tweet).encode('utf8') + b\"\\n\")\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this code after alteration to convert your downloaded tweet id files to a manageable number of ids\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "#path = os.getcwd()\n",
    "\n",
    "humza = pd.read_csv(\"C:\\\\Users\\\\Afnan Anwar\\\\Desktop\\\\SPROJ\\\\Cleaning\\\\2020-11\\\\Election.txt\")\n",
    "humza.columns = ['id'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = list(humza.loc[:10000,'id'])\n",
    "os.chdir(\"C:\\\\Users\\\\Afnan Anwar\\\\Desktop\\\\SPROJ\\\\Cleaning\\\\2020-11\")\n",
    "\n",
    "new_file = open('target.txt','w')\n",
    "\n",
    "for id in target:\n",
    "    new_file.write(str(id)+\"\\n\")\n",
    "    \n",
    "new_file.close()\n",
    "#len(humza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this after running the second cell which hydrated the ids. Use the directory named based on folder name where you stored id file. \n",
    "# Format for directory is YYYY-MM\n",
    "\n",
    "path = 'C:\\\\Users\\\\Afnan Anwar\\\\Desktop\\\\SPROJ\\\\Cleaning'\n",
    "all_files = glob.glob(path + \"/*.gz\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_json(filename,compression='infer',lines=True)\n",
    "    li.append(df)\n",
    "\n",
    "data = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import glob\n",
    "import os\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The keywords used to extract this data were Trump, Biden and elections2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8548"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking how many unique users\n",
    "len(data.id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.created_at = pd.to_datetime(data.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.477772578380907"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Litchman's key : social unrest\n",
    "temp_frame = data.copy()\n",
    "temp_frame['full_text'] = temp_frame['full_text'].str.lower() \n",
    "unrest = temp_frame[temp_frame.full_text.str.contains('unrest')]\n",
    "protest = temp_frame[temp_frame.full_text.str.contains('protest')]\n",
    "violence = temp_frame[temp_frame.full_text.str.contains('violence')]\n",
    "demonstration = temp_frame[temp_frame.full_text.str.contains('demonstration')]\n",
    "turmoil = temp_frame[temp_frame.full_text.str.contains('turmoil')]\n",
    "outcry = temp_frame[temp_frame.full_text.str.contains('outcry')]\n",
    "riot = temp_frame[temp_frame.full_text.str.contains('riot')]\n",
    "revolt = temp_frame[temp_frame.full_text.str.contains('revolt')]\n",
    "Floyd = temp_frame[temp_frame.full_text.str.contains('floyd')]\n",
    "George  = temp_frame[temp_frame.full_text.str.contains('George')]\n",
    "immigrants = temp_frame[temp_frame.full_text.str.contains('immigrant')]\n",
    "muslims = temp_frame[temp_frame.full_text.str.contains('muslim')]\n",
    "protest  = temp_frame[temp_frame.full_text.str.contains('protest')]\n",
    "BLM = temp_frame[temp_frame.full_text.str.contains('BLM')]\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#print(f\"The number of tweets containing unrest are {len(unrest)}, The number of tweets containing protest are {len(protest)}, The number of tweets containing demonstration are {len(demonstration)}, The number of tweets containing turmoil are {len(turmoil)}, The number of tweets containing outcry are {len(outcry)}, The number of tweets containing riot are {len(riot)}, The number of tweets containing revolt are {len(revolt)}\")\n",
    "unrest =  len(unrest)+ len(protest)+ len(demonstration)+ len(turmoil) + len(outcry) + len(riot) + len(revolt)+ len(Floyd)+len(George)+len(immigrants)+len(muslims)+len(protest)+len(BLM)\n",
    "\n",
    "unrest\n",
    "\n",
    "topicdominance = (unrest/8548)*100\n",
    "#topicdominance\n",
    "\n",
    "#Litchman's key: Mid-term gains\n",
    "#temp_frame['full_text'] = temp_frame['full_text'].str.lower() \n",
    "\n",
    "democrats = temp_frame[temp_frame.full_text.str.contains('democrats')]\n",
    "republicans = temp_frame[temp_frame.full_text.str.contains('republicans')]\n",
    "seats = temp_frame[temp_frame.full_text.str.contains('seats')]\n",
    "speaker = temp_frame[temp_frame.full_text.str.contains('Nancy Pelosi')]\n",
    "congress = temp_frame[temp_frame.full_text.str.contains('congress')]\n",
    "\n",
    "mtg =  len(democrats)+ len(republicans)+ len(seats)+ len(speaker) + len(congress)\n",
    "\n",
    "\n",
    "#mtg\n",
    "\n",
    "topdom2 = (mtg/8548)*100\n",
    "#topdom2\n",
    "\n",
    "\n",
    "\n",
    "#Litchman's key: No third party\n",
    "party = temp_frame[temp_frame.full_text.str.contains('party')]\n",
    "third = temp_frame[temp_frame.full_text.str.contains('third')]\n",
    "libertarians = temp_frame[temp_frame.full_text.str.contains('Libertarian')]\n",
    "greens= temp_frame[temp_frame.full_text.str.contains('Greens')]\n",
    "bi_partisan = temp_frame[temp_frame.full_text.str.contains('bipartisan')]\n",
    "\n",
    "\n",
    "ntp =  len(party)+ len(third)+ len(libertarians)+ len(greens) + len(bi_partisan) \n",
    "#ntp\n",
    "\n",
    "ntpdom = (ntp/8548)*100\n",
    "#ntpdom\n",
    "\n",
    "# Litchman's key: No primary contest\n",
    "\n",
    "primary = temp_frame[temp_frame.full_text.str.contains('primary')]\n",
    "contest = temp_frame[temp_frame.full_text.str.contains('contest')]\n",
    "rocky_de_la_fuente = temp_frame[temp_frame.full_text.str.contains('Rocky De La Fuente')]\n",
    "caucuses = temp_frame[temp_frame.full_text.str.contains('Caucuses')]\n",
    "\n",
    "\n",
    "prim_contest =  len(primary)+ len(contest)+ len(rocky_de_la_fuente)+ len(caucuses)\n",
    "#prim_contest\n",
    "\n",
    "pc = (prim_contest/8548)*100\n",
    "#pc\n",
    "\n",
    "#Litchman's key: Incumbent seeking re-election\n",
    "reelection = temp_frame[temp_frame.full_text.str.contains('re elect')]\n",
    "four_more_years = temp_frame[temp_frame.full_text.str.contains('4 more')]\n",
    "keep_america_great = temp_frame[temp_frame.full_text.str.contains('keep America')]\n",
    "MAGA = keep_america_great = temp_frame[temp_frame.full_text.str.contains('MAGA')]\n",
    "Trump2020 = temp_frame[temp_frame.full_text.str.contains('Trump')]\n",
    "great = temp_frame[temp_frame.full_text.str.contains('great')]\n",
    "\n",
    "incumbent =  len(reelection)+ len(four_more_years)+ len(keep_america_great)+ len(MAGA)+ len(Trump2020)+len(great)\n",
    "#incumbent\n",
    "\n",
    "reelection = (incumbent/8548)*100\n",
    "#reelection\n",
    "\n",
    "#Litchman's key: Strong short-term economy\n",
    "unemployment = temp_frame[temp_frame.full_text.str.contains('unemployment')]\n",
    "recession = temp_frame[temp_frame.full_text.str.contains('recession')]\n",
    "stock = temp_frame[temp_frame.full_text.str.contains('stock')]\n",
    "economy = temp_frame[temp_frame.full_text.str.contains('economy')]\n",
    "market = temp_frame[temp_frame.full_text.str.contains('market')]\n",
    "trade = temp_frame[temp_frame.full_text.str.contains('trade')]\n",
    "\n",
    "short_term =  len(unemployment)+ len(recession)+ len(stock)+ len(economy)+ len(market)+len(trade)\n",
    "#short_term\n",
    "\n",
    "st = (short_term/8548)*100\n",
    "#st\n",
    "\n",
    "\n",
    "\n",
    "# Strong long-term economy\n",
    "GDP = temp_frame[temp_frame.full_text.str.contains('GDP')]\n",
    "economy = temp_frame[temp_frame.full_text.str.contains('economy')]\n",
    "growth = temp_frame[temp_frame.full_text.str.contains('growth')]\n",
    "boom = temp_frame[temp_frame.full_text.str.contains('boom')]\n",
    "market = temp_frame[temp_frame.full_text.str.contains('market')]\n",
    "shares = temp_frame[temp_frame.full_text.str.contains('shares')]\n",
    "stocks = temp_frame[temp_frame.full_text.str.contains('stocks')]\n",
    "\n",
    "long_term =  len(GDP)+ len(economy)+ len(growth)+ len(boom)+ len(market)+len(shares)+len(stocks)\n",
    "\n",
    "lt = (long_term/8548)*100\n",
    "\n",
    "lt\n",
    "# Major policy change.\n",
    "executive = temp_frame[temp_frame.full_text.str.contains('executive')]\n",
    "order = temp_frame[temp_frame.full_text.str.contains('order')]\n",
    "Obama_care = temp_frame[temp_frame.full_text.str.contains('Obama care')]\n",
    "fracking  = temp_frame[temp_frame.full_text.str.contains('fracking')]\n",
    "environment = temp_frame[temp_frame.full_text.str.contains('environment')]\n",
    "Health_care = temp_frame[temp_frame.full_text.str.contains('Health')]\n",
    "Care= temp_frame[temp_frame.full_text.str.contains('Care')]\n",
    "peace = temp_frame[temp_frame.full_text.str.contains('peace')]\n",
    "funding = temp_frame[temp_frame.full_text.str.contains('funding')]\n",
    "immigration = temp_frame[temp_frame.full_text.str.contains('immigration')]\n",
    "Supreme = temp_frame[temp_frame.full_text.str.contains('Supreme Court')]\n",
    "tax = temp_frame[temp_frame.full_text.str.contains('tax')]\n",
    "\n",
    "mpc =  len(executive)+ len(order)+ len(Obama_care)+ len(fracking)+ len(environment)+len(Health_care)+len(Care)+len(peace)+len(funding)+len(immigration)+ len(Supreme)+len(tax)\n",
    "#mpc\n",
    "\n",
    "mdom = (mpc/8548)*100\n",
    "#mdom\n",
    "\n",
    "#No scandal\n",
    "impeachment = temp_frame[temp_frame.full_text.str.contains('impeachment')]\n",
    "scandal   = temp_frame[temp_frame.full_text.str.contains('scandal')]\n",
    "Ukraine = temp_frame[temp_frame.full_text.str.contains('ukraine')]\n",
    "transcript  = temp_frame[temp_frame.full_text.str.contains('transcript')]\n",
    "interference    = temp_frame[temp_frame.full_text.str.contains('interference')]\n",
    "call = temp_frame[temp_frame.full_text.str.contains('call')]\n",
    "Hunter = temp_frame[temp_frame.full_text.str.contains('Hunter')]\n",
    "Biden = temp_frame[temp_frame.full_text.str.contains('Biden')]\n",
    "allegation = temp_frame[temp_frame.full_text.str.contains('allegation')]\n",
    "impeachable = temp_frame[temp_frame.full_text.str.contains('impeachable')]\n",
    "aid  = temp_frame[temp_frame.full_text.str.contains('aid')]\n",
    "whistleblower = temp_frame[temp_frame.full_text.str.contains('whistleblower')]\n",
    "controversy = temp_frame[temp_frame.full_text.str.contains('controversy')]\n",
    "\n",
    "\n",
    "ns =  len(impeachment)+len(scandal)+len(Ukraine)+len(transcript)+len(interference)+len(call)+len(Hunter)+len(Biden)+len(allegation)+len(impeachable)+len(aid)+len(whistleblower)+len(controversy)\n",
    "\n",
    "nsdom = (ns/8548)*100\n",
    "nsdom\n",
    "# No foreign/military failure\n",
    "china = temp_frame[temp_frame.full_text.str.contains('china')]\n",
    "northkorea = temp_frame[temp_frame.full_text.str.contains('north korea')]\n",
    "russia   = temp_frame[temp_frame.full_text.str.contains('russia')]\n",
    "iran = temp_frame[temp_frame.full_text.str.contains('iran')]\n",
    "bombing = temp_frame[temp_frame.full_text.str.contains('bombing')]\n",
    "middle_east = temp_frame[temp_frame.full_text.str.contains('middle east')]\n",
    "WW3 = temp_frame[temp_frame.full_text.str.contains('WW3')]\n",
    "war = temp_frame[temp_frame.full_text.str.contains('war')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "military = len(china)+len(northkorea)+len(russia)+len(peace)+len(iran)+len(bombing)+len(middle_east)+len(WW3)+len(war)\n",
    "m = (military/8548)*100\n",
    "#m\n",
    "\n",
    "# Military success\n",
    "nato = temp_frame[temp_frame.full_text.str.contains('nato')]\n",
    "nobel = temp_frame[temp_frame.full_text.str.contains('Nobel')]\n",
    "prize = temp_frame[temp_frame.full_text.str.contains('prize')]\n",
    "peace = temp_frame[temp_frame.full_text.str.contains('peace')]\n",
    "Israel = temp_frame[temp_frame.full_text.str.contains('Israel')]\n",
    "\n",
    "\n",
    "ms = len(nato) +len(nobel)+len(prize)+len(peace)+len(Israel)\n",
    "\n",
    "msdom = (ms/8548)*100\n",
    "msdom\n",
    "\n",
    "#Charismatic incumbent\n",
    "atinfa = temp_frame[temp_frame.full_text.str.contains('antifa')]\n",
    "Anon = temp_frame[temp_frame.full_text.str.contains('anon')]\n",
    "Q = temp_frame[temp_frame.full_text.str.contains('Q')]\n",
    "cult = temp_frame[temp_frame.full_text.str.contains('cult')]\n",
    "great = temp_frame[temp_frame.full_text.str.contains('great')]\n",
    "powerful  = temp_frame[temp_frame.full_text.str.contains('powerful')]\n",
    "leader = temp_frame[temp_frame.full_text.str.contains('leader')]\n",
    "\n",
    "ch = len(atinfa)+len(Anon)+len(Q)+len(cult)+len(great)+len(powerful)+len(leader)\n",
    "\n",
    "chdom = (ms/8548)*100\n",
    "chdom\n",
    "\n",
    "#Uncharismatic challenger\n",
    "\n",
    "sleepy = temp_frame[temp_frame.full_text.str.contains('sleepy')]\n",
    "joe = temp_frame[temp_frame.full_text.str.contains('joe')]\n",
    "basement = temp_frame[temp_frame.full_text.str.contains('basement')]\n",
    "Quid_Pro= temp_frame[temp_frame.full_text.str.contains('quid Pro')]\n",
    "slow = temp_frame[temp_frame.full_text.str.contains('slow')]\n",
    "creepy = temp_frame[temp_frame.full_text.str.contains('creepy')]\n",
    "old = temp_frame[temp_frame.full_text.str.contains('old')]\n",
    "violent = temp_frame[temp_frame.full_text.str.contains('violent')]\n",
    "socialist = temp_frame[temp_frame.full_text.str.contains('socialist')]\n",
    "stutter = temp_frame[temp_frame.full_text.str.contains('stutter')]\n",
    "stammer = temp_frame[temp_frame.full_text.str.contains('stammer')]\n",
    "\n",
    "\n",
    "unch = len(sleepy)+len(joe)+ len(basement)+len(Quid_Pro)+len(slow)+len(creepy)+len(old)+len(violent)+len(socialist)+len(stutter)+len(stammer)\n",
    "\n",
    "undom = (unch/8548)*100\n",
    "undom\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets containing Trump are 4998 while those containing Biden are 2013.\n"
     ]
    }
   ],
   "source": [
    "# Segmenting Data based on tweet text containing Trump and Biden \n",
    "\n",
    "temp_frame['full_text'] = temp_frame['full_text'].str.lower() \n",
    "trump = temp_frame[temp_frame.full_text.str.contains('trump')]\n",
    "biden = temp_frame[temp_frame.full_text.str.contains('biden')]\n",
    "\n",
    "print(f\"The number of tweets containing Trump are {len(trump)} while those containing Biden are {len(biden)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>full_text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>entities</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>source</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>...</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>lang</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>quoted_status_permalink</th>\n",
       "      <th>quoted_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-02 22:59:55+00:00</td>\n",
       "      <td>1323399466439892999</td>\n",
       "      <td>1323399466439892992</td>\n",
       "      <td>rt @realdonaldtrump: https://t.co/cnrzjewxko</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 44]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>{'media': [{'id': 1323351620948762624, 'id_str...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6390</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>und</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-02 22:59:55+00:00</td>\n",
       "      <td>1323399467064872961</td>\n",
       "      <td>1323399467064872960</td>\n",
       "      <td>rt @realdonaldtrump: just landed in traverse c...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 71]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20829</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-02 22:59:55+00:00</td>\n",
       "      <td>1323399467140292608</td>\n",
       "      <td>1323399467140292608</td>\n",
       "      <td>rt @realdonaldtrump: i prepaid millions of dol...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 75]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20714</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-02 22:59:55+00:00</td>\n",
       "      <td>1323399467379396608</td>\n",
       "      <td>1323399467379396608</td>\n",
       "      <td>rt @realdonaldtrump: as christians throughout ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 140]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>25536</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-02 22:59:55+00:00</td>\n",
       "      <td>1323399467408777216</td>\n",
       "      <td>1323399467408777216</td>\n",
       "      <td>rt @realdonaldtrump: just landed in ohio. see ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 68]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>13057</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8543</th>\n",
       "      <td>2020-11-02 23:02:30+00:00</td>\n",
       "      <td>1323400119773880320</td>\n",
       "      <td>1323400119773880320</td>\n",
       "      <td>rt @chrisklemens: trump supporters are a cance...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 65]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1590</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8544</th>\n",
       "      <td>2020-11-02 23:02:30+00:00</td>\n",
       "      <td>1323400119887298561</td>\n",
       "      <td>1323400119887298560</td>\n",
       "      <td>rt @realdonaldtrump: ¡mi #americandreamplan es...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 140]</td>\n",
       "      <td>{'hashtags': [{'text': 'AmericanDreamPlan', 'i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>18032</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8545</th>\n",
       "      <td>2020-11-02 23:02:30+00:00</td>\n",
       "      <td>1323400120004739072</td>\n",
       "      <td>1323400120004739072</td>\n",
       "      <td>rt @realdonaldtrump: make america great again!</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 46]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>47083</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8546</th>\n",
       "      <td>2020-11-02 23:02:30+00:00</td>\n",
       "      <td>1323400120013103106</td>\n",
       "      <td>1323400120013103104</td>\n",
       "      <td>rt @realdonaldtrump: my #americandreamplan is ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 140]</td>\n",
       "      <td>{'hashtags': [{'text': 'AmericanDreamPlan', 'i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>13776</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8547</th>\n",
       "      <td>2020-11-02 23:02:30+00:00</td>\n",
       "      <td>1323400120436772878</td>\n",
       "      <td>1323400120436772864</td>\n",
       "      <td>rt @realdonaldtrump: just landed in traverse c...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 71]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20830</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8548 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    created_at                   id               id_str  \\\n",
       "0    2020-11-02 22:59:55+00:00  1323399466439892999  1323399466439892992   \n",
       "1    2020-11-02 22:59:55+00:00  1323399467064872961  1323399467064872960   \n",
       "2    2020-11-02 22:59:55+00:00  1323399467140292608  1323399467140292608   \n",
       "3    2020-11-02 22:59:55+00:00  1323399467379396608  1323399467379396608   \n",
       "4    2020-11-02 22:59:55+00:00  1323399467408777216  1323399467408777216   \n",
       "...                        ...                  ...                  ...   \n",
       "8543 2020-11-02 23:02:30+00:00  1323400119773880320  1323400119773880320   \n",
       "8544 2020-11-02 23:02:30+00:00  1323400119887298561  1323400119887298560   \n",
       "8545 2020-11-02 23:02:30+00:00  1323400120004739072  1323400120004739072   \n",
       "8546 2020-11-02 23:02:30+00:00  1323400120013103106  1323400120013103104   \n",
       "8547 2020-11-02 23:02:30+00:00  1323400120436772878  1323400120436772864   \n",
       "\n",
       "                                              full_text  truncated  \\\n",
       "0          rt @realdonaldtrump: https://t.co/cnrzjewxko      False   \n",
       "1     rt @realdonaldtrump: just landed in traverse c...      False   \n",
       "2     rt @realdonaldtrump: i prepaid millions of dol...      False   \n",
       "3     rt @realdonaldtrump: as christians throughout ...      False   \n",
       "4     rt @realdonaldtrump: just landed in ohio. see ...      False   \n",
       "...                                                 ...        ...   \n",
       "8543  rt @chrisklemens: trump supporters are a cance...      False   \n",
       "8544  rt @realdonaldtrump: ¡mi #americandreamplan es...      False   \n",
       "8545     rt @realdonaldtrump: make america great again!      False   \n",
       "8546  rt @realdonaldtrump: my #americandreamplan is ...      False   \n",
       "8547  rt @realdonaldtrump: just landed in traverse c...      False   \n",
       "\n",
       "     display_text_range                                           entities  \\\n",
       "0               [0, 44]  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "1               [0, 71]  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "2               [0, 75]  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "3              [0, 140]  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "4               [0, 68]  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "...                 ...                                                ...   \n",
       "8543            [0, 65]  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "8544           [0, 140]  {'hashtags': [{'text': 'AmericanDreamPlan', 'i...   \n",
       "8545            [0, 46]  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "8546           [0, 140]  {'hashtags': [{'text': 'AmericanDreamPlan', 'i...   \n",
       "8547            [0, 71]  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "\n",
       "                                      extended_entities  \\\n",
       "0     {'media': [{'id': 1323351620948762624, 'id_str...   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "8543                                                NaN   \n",
       "8544                                                NaN   \n",
       "8545                                                NaN   \n",
       "8546                                                NaN   \n",
       "8547                                                NaN   \n",
       "\n",
       "                                                 source  \\\n",
       "0     <a href=\"http://twitter.com/download/android\" ...   \n",
       "1     <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "2     <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "3     <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "4     <a href=\"http://twitter.com/download/android\" ...   \n",
       "...                                                 ...   \n",
       "8543  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "8544  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "8545  <a href=\"http://twitter.com/download/android\" ...   \n",
       "8546  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "8547  <a href=\"http://twitter.com/download/android\" ...   \n",
       "\n",
       "      in_reply_to_status_id  ...  retweet_count  favorite_count  favorited  \\\n",
       "0                       NaN  ...           6390               0      False   \n",
       "1                       NaN  ...          20829               0      False   \n",
       "2                       NaN  ...          20714               0      False   \n",
       "3                       NaN  ...          25536               0      False   \n",
       "4                       NaN  ...          13057               0      False   \n",
       "...                     ...  ...            ...             ...        ...   \n",
       "8543                    NaN  ...           1590               0      False   \n",
       "8544                    NaN  ...          18032               0      False   \n",
       "8545                    NaN  ...          47083               0      False   \n",
       "8546                    NaN  ...          13776               0      False   \n",
       "8547                    NaN  ...          20830               0      False   \n",
       "\n",
       "     retweeted possibly_sensitive lang quoted_status_id quoted_status_id_str  \\\n",
       "0        False                0.0  und              NaN                  NaN   \n",
       "1        False                NaN   en              NaN                  NaN   \n",
       "2        False                NaN   en              NaN                  NaN   \n",
       "3        False                NaN   en              NaN                  NaN   \n",
       "4        False                NaN   en              NaN                  NaN   \n",
       "...        ...                ...  ...              ...                  ...   \n",
       "8543     False                NaN   en              NaN                  NaN   \n",
       "8544     False                NaN   es              NaN                  NaN   \n",
       "8545     False                NaN   en              NaN                  NaN   \n",
       "8546     False                NaN   en              NaN                  NaN   \n",
       "8547     False                NaN   en              NaN                  NaN   \n",
       "\n",
       "      quoted_status_permalink quoted_status  \n",
       "0                         NaN           NaN  \n",
       "1                         NaN           NaN  \n",
       "2                         NaN           NaN  \n",
       "3                         NaN           NaN  \n",
       "4                         NaN           NaN  \n",
       "...                       ...           ...  \n",
       "8543                      NaN           NaN  \n",
       "8544                      NaN           NaN  \n",
       "8545                      NaN           NaN  \n",
       "8546                      NaN           NaN  \n",
       "8547                      NaN           NaN  \n",
       "\n",
       "[8548 rows x 31 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grouping based on Unique Users\n",
    "temp_frame.columns\n",
    "# Dropping duplicates\n",
    "temp_frame = temp_frame.drop_duplicates('id')\n",
    "temp_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Afnan\n",
      "[nltk_data]     Anwar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "#from wordcloud import WordCloud\n",
    "# Plotting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Taking a look at the data\n",
    "temp_frame.head()\n",
    "\n",
    "#Checking the col names\n",
    "#for col in df.columns: \n",
    "#    print(col)\n",
    "\n",
    "#Checking out the 'text' col\n",
    "    #print(df['text'])\n",
    "\n",
    "#Remove '@ User from the 'text' col\n",
    "    \n",
    "def remove_users(tweet, pattern1, pattern2):\n",
    "    r = re.findall(pattern1, tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "\n",
    "    r = re.findall(pattern2, tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "    return tweet\n",
    "\n",
    "df['tidy_tweet'] = df['full_text'].apply(lambda x: remove_users(x,\"@ [\\w]*\", \"@[\\w]*\"))\n",
    "\n",
    "#Normalization\n",
    "df['tidy_tweet'] = df['tidy_tweet'].str.lower()\n",
    "#print(df['tidy_tweet'])\n",
    "\n",
    "# Remove all the hashtags from the text\n",
    "def remove_hashtags(tweet, pattern1, pattern2):\n",
    "    r = re.findall(pattern1, tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "\n",
    "    r = re.findall(pattern2, tweet)\n",
    "    for i in r:\n",
    "        tweet = re.sub(i, '', tweet)\n",
    "    return tweet\n",
    "\n",
    "df['tidy_tweet'] = df['tidy_tweet'].apply(lambda x: remove_hashtags(x,\"# [\\w]*\", \"#[\\w]*\"))\n",
    "\n",
    "# Remove all links & URLs\n",
    "def remove_links(tweet):\n",
    "    tweet_no_link = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    return tweet_no_link\n",
    "\n",
    "df['tidy_tweet'] = df['tidy_tweet'].apply(remove_links)\n",
    "\n",
    "# Removing Punctuations, Numbers, and Special Characters\n",
    "df['tidy_tweet'] = df['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "#Removing words with less than 3 characters\n",
    "df['tidy_tweet'] = df['tidy_tweet'].apply(lambda x:' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "#Tokenization\n",
    "def tokenize(tweet):\n",
    "    for word in tweet:\n",
    "        yield(gensim.utils.simple_preprocess(str(word), deacc=True))  \n",
    "\n",
    "df['tidy_tweet_tokens'] = list(tokenize(df['tidy_tweet']))\n",
    "\n",
    "# Prepare Stop Words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'https', 'twitter', 'religions', 'pic','twitt',])\n",
    "\n",
    "# REMOVE STOPWORDS\n",
    "def remove_stopwords(tweets):\n",
    "    return [[word for word in simple_preprocess(str(tweet)) if word not in stop_words] for tweet in tweets]\n",
    "\n",
    "df['tokens_no_stop'] = remove_stopwords(df['tidy_tweet_tokens'])\n",
    "\n",
    "# REMOVE TWEETS LESS THAN 3 TOKENS\n",
    "df['length'] = df['tokens_no_stop'].apply(len)\n",
    "df = df.drop(df[df['length']<3].index)\n",
    "df = df.drop(['length'], axis=1)\n",
    "\n",
    "df = df[['created_at','tokens_no_stop','full_text',]]\n",
    "#df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Calculating Compound Sentiment Scores for Social unrest\n",
    "\n",
    "# Vader Sentiment function\n",
    "\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    return score['compound']\n",
    "\n",
    "# The following two lines will take ages to execute on millions of tweets so I'd suggest run it on a couple 100 with slicing to see if this works for you guys\n",
    "\n",
    "unrest_sent = unrest['full_text'][:100].map(sentiment_analyzer_scores)\n",
    "protest_sent = protest['full_text'][:100].map(sentiment_analyzer_scores)\n",
    "turmoil_sent = turmoil['full_text'][:100].map(sentiment_analyzer_scores)\n",
    "demonstration_sent = demonstration['full_text'][:100].map(sentiment_analyzer_scores)\n",
    "outcry_sent = outcry['full_text'][:100].map(sentiment_analyzer_scores)\n",
    "riot_sent = riot['full_text'][:100].map(sentiment_analyzer_scores)\n",
    "revolt_sent = revolt['full_text'][:100].map(sentiment_analyzer_scores)\n",
    "'''\n",
    "# Calculating Compound Sentiment Scores for Trump and Biden\n",
    "\n",
    "# Vader Sentiment function\n",
    "\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    return score['compound']\n",
    "\n",
    "# The following two lines will take ages to execute on millions of tweets so I'd suggest run it on a couple 100 with slicing to see if this works for you guys\n",
    "\n",
    "trump_sent = trump['full_text'][:100].map(sentiment_analyzer_scores)\n",
    "biden_sent = biden['full_text'][:100].map(sentiment_analyzer_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words Conversion\n",
    "dictionary = gensim.corpora.Dictionary(df['tokens_no_stop'])\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(tweet) for tweet in df['tokens_no_stop']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    }
   ],
   "source": [
    "# Topic Modelling using BOW\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=25, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic : 0 \n",
      "Words: 0.055*\"biden\" + 0.028*\"election\" + 0.024*\"america\" + 0.021*\"like\" + 0.020*\"state\" + 0.019*\"trump\" + 0.017*\"home\" + 0.015*\"people\" + 0.014*\"prison\" + 0.014*\"vaccine\"\n",
      "\n",
      "Topic : 1 \n",
      "Words: 0.055*\"people\" + 0.051*\"trump\" + 0.043*\"biden\" + 0.041*\"america\" + 0.034*\"great\" + 0.030*\"make\" + 0.022*\"american\" + 0.022*\"time\" + 0.018*\"vote\" + 0.017*\"donald\"\n",
      "\n",
      "Topic : 2 \n",
      "Words: 0.089*\"trump\" + 0.029*\"biden\" + 0.027*\"covid\" + 0.025*\"lebron\" + 0.025*\"spreader\" + 0.025*\"james\" + 0.021*\"school\" + 0.020*\"obama\" + 0.020*\"children\" + 0.019*\"gave\"\n",
      "\n",
      "Topic : 3 \n",
      "Words: 0.087*\"trump\" + 0.046*\"country\" + 0.042*\"great\" + 0.039*\"remember\" + 0.038*\"built\" + 0.038*\"christians\" + 0.037*\"souls\" + 0.036*\"went\" + 0.035*\"celebrate\" + 0.035*\"throughout\"\n",
      "\n",
      "Topic : 4 \n",
      "Words: 0.102*\"biden\" + 0.052*\"harris\" + 0.048*\"kamala\" + 0.018*\"china\" + 0.017*\"communism\" + 0.016*\"calls\" + 0.015*\"michigan\" + 0.015*\"equality\" + 0.014*\"mocked\" + 0.014*\"outcome\"\n",
      "\n",
      "Topic : 5 \n",
      "Words: 0.072*\"americans\" + 0.070*\"education\" + 0.069*\"economy\" + 0.069*\"hispanic\" + 0.069*\"promise\" + 0.069*\"opportunity\" + 0.069*\"provide\" + 0.068*\"fuel\" + 0.068*\"thriving\" + 0.017*\"biden\"\n",
      "\n",
      "Topic : 6 \n",
      "Words: 0.133*\"trump\" + 0.040*\"president\" + 0.020*\"obama\" + 0.018*\"america\" + 0.017*\"voting\" + 0.015*\"rally\" + 0.015*\"years\" + 0.014*\"year\" + 0.014*\"left\" + 0.013*\"voted\"\n",
      "\n",
      "Topic : 7 \n",
      "Words: 0.047*\"trump\" + 0.037*\"vote\" + 0.033*\"biden\" + 0.025*\"ballot\" + 0.021*\"voting\" + 0.019*\"millions\" + 0.018*\"true\" + 0.018*\"best\" + 0.017*\"votes\" + 0.017*\"going\"\n",
      "\n",
      "Topic : 8 \n",
      "Words: 0.070*\"trump\" + 0.058*\"vote\" + 0.028*\"ballot\" + 0.024*\"america\" + 0.022*\"choose\" + 0.020*\"people\" + 0.020*\"supporters\" + 0.016*\"election\" + 0.015*\"voting\" + 0.014*\"biden\"\n",
      "\n",
      "Topic : 9 \n",
      "Words: 0.056*\"vote\" + 0.045*\"trump\" + 0.031*\"biden\" + 0.024*\"know\" + 0.022*\"election\" + 0.022*\"dems\" + 0.021*\"president\" + 0.020*\"plan\" + 0.019*\"lead\" + 0.019*\"enough\"\n",
      "\n",
      "Topic : 10 \n",
      "Words: 0.100*\"michigan\" + 0.099*\"city\" + 0.097*\"traverse\" + 0.094*\"crowd\" + 0.092*\"landed\" + 0.056*\"support\" + 0.037*\"vote\" + 0.028*\"military\" + 0.027*\"protect\" + 0.027*\"trump\"\n",
      "\n",
      "Topic : 11 \n",
      "Words: 0.119*\"trump\" + 0.041*\"donald\" + 0.036*\"biden\" + 0.025*\"president\" + 0.020*\"watch\" + 0.016*\"wall\" + 0.016*\"would\" + 0.016*\"ever\" + 0.014*\"election\" + 0.012*\"lies\"\n",
      "\n",
      "Topic : 12 \n",
      "Words: 0.073*\"great\" + 0.073*\"remember\" + 0.072*\"country\" + 0.070*\"went\" + 0.070*\"christians\" + 0.070*\"built\" + 0.069*\"celebrate\" + 0.069*\"souls\" + 0.069*\"throughout\" + 0.014*\"biden\"\n",
      "\n",
      "Topic : 13 \n",
      "Words: 0.073*\"answer\" + 0.041*\"trying\" + 0.038*\"know\" + 0.038*\"force\" + 0.037*\"stop\" + 0.037*\"traverse\" + 0.037*\"city\" + 0.037*\"michigan\" + 0.037*\"beautiful\" + 0.036*\"depraved\"\n",
      "\n",
      "Topic : 14 \n",
      "Words: 0.055*\"biden\" + 0.036*\"campaign\" + 0.032*\"trump\" + 0.030*\"much\" + 0.028*\"make\" + 0.026*\"long\" + 0.024*\"every\" + 0.023*\"final\" + 0.022*\"hours\" + 0.021*\"sure\"\n",
      "\n",
      "Topic : 15 \n",
      "Words: 0.067*\"biden\" + 0.067*\"vote\" + 0.055*\"trump\" + 0.024*\"wins\" + 0.021*\"donald\" + 0.014*\"tomorrow\" + 0.014*\"voting\" + 0.014*\"election\" + 0.013*\"make\" + 0.013*\"know\"\n",
      "\n",
      "Topic : 16 \n",
      "Words: 0.066*\"trump\" + 0.044*\"election\" + 0.037*\"biden\" + 0.025*\"white\" + 0.022*\"wins\" + 0.021*\"proud\" + 0.020*\"gonna\" + 0.020*\"people\" + 0.019*\"protestors\" + 0.018*\"boys\"\n",
      "\n",
      "Topic : 17 \n",
      "Words: 0.078*\"para\" + 0.071*\"econom\" + 0.071*\"proveer\" + 0.071*\"oportunida\" + 0.071*\"promesa\" + 0.071*\"spera\" + 0.071*\"impulsar\" + 0.018*\"trump\" + 0.016*\"biden\" + 0.011*\"people\"\n",
      "\n",
      "Topic : 18 \n",
      "Words: 0.062*\"trump\" + 0.055*\"president\" + 0.031*\"biden\" + 0.028*\"vote\" + 0.022*\"never\" + 0.021*\"need\" + 0.019*\"pennsylvania\" + 0.016*\"like\" + 0.014*\"want\" + 0.012*\"america\"\n",
      "\n",
      "Topic : 19 \n",
      "Words: 0.068*\"biden\" + 0.041*\"back\" + 0.035*\"build\" + 0.023*\"trump\" + 0.021*\"economy\" + 0.020*\"year\" + 0.020*\"going\" + 0.019*\"together\" + 0.018*\"better\" + 0.017*\"rebuild\"\n",
      "\n",
      "Topic : 20 \n",
      "Words: 0.062*\"america\" + 0.054*\"order\" + 0.053*\"stop\" + 0.052*\"students\" + 0.052*\"radical\" + 0.051*\"signed\" + 0.051*\"commission\" + 0.048*\"indoctrination\" + 0.048*\"establish\" + 0.030*\"heard\"\n",
      "\n",
      "Topic : 21 \n",
      "Words: 0.066*\"trump\" + 0.037*\"federal\" + 0.035*\"income\" + 0.034*\"dollars\" + 0.033*\"taxes\" + 0.030*\"prepaid\" + 0.030*\"millions\" + 0.021*\"para\" + 0.014*\"real\" + 0.012*\"econom\"\n",
      "\n",
      "Topic : 22 \n",
      "Words: 0.036*\"trump\" + 0.027*\"votes\" + 0.026*\"working\" + 0.024*\"biden\" + 0.021*\"think\" + 0.019*\"vote\" + 0.018*\"american\" + 0.018*\"pittsburgh\" + 0.018*\"look\" + 0.018*\"overtime\"\n",
      "\n",
      "Topic : 23 \n",
      "Words: 0.059*\"trump\" + 0.023*\"lebron\" + 0.021*\"biden\" + 0.019*\"ballots\" + 0.017*\"michigan\" + 0.015*\"vote\" + 0.015*\"every\" + 0.014*\"james\" + 0.014*\"landslide\" + 0.013*\"reminder\"\n",
      "\n",
      "Topic : 24 \n",
      "Words: 0.080*\"trump\" + 0.079*\"biden\" + 0.045*\"vote\" + 0.030*\"tomorrow\" + 0.024*\"going\" + 0.020*\"please\" + 0.014*\"take\" + 0.013*\"lady\" + 0.013*\"people\" + 0.012*\"last\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic : {idx} \\nWords: {topic}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7599964141845703\t \n",
      "Topic: 0.115*\"trump\" + 0.062*\"biden\" + 0.026*\"lady\" + 0.025*\"gaga\" + 0.020*\"pennsylvania\" + 0.019*\"millions\" + 0.016*\"covid\" + 0.015*\"votes\" + 0.014*\"election\" + 0.013*\"deaths\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.173*\"vote\" + 0.058*\"trump\" + 0.042*\"biden\" + 0.017*\"tomorrow\" + 0.016*\"proud\" + 0.014*\"ballot\" + 0.011*\"wins\" + 0.011*\"police\" + 0.011*\"voting\" + 0.011*\"gonna\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.073*\"country\" + 0.071*\"great\" + 0.069*\"remember\" + 0.068*\"christians\" + 0.068*\"souls\" + 0.068*\"built\" + 0.067*\"went\" + 0.067*\"celebrate\" + 0.067*\"throughout\" + 0.035*\"biden\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.049*\"president\" + 0.045*\"trump\" + 0.025*\"biden\" + 0.021*\"year\" + 0.021*\"every\" + 0.020*\"remember\" + 0.019*\"great\" + 0.019*\"voting\" + 0.019*\"country\" + 0.016*\"vote\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.078*\"trump\" + 0.054*\"biden\" + 0.030*\"harris\" + 0.020*\"kamala\" + 0.018*\"states\" + 0.018*\"lebron\" + 0.014*\"united\" + 0.012*\"vote\" + 0.012*\"support\" + 0.011*\"election\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.048*\"trump\" + 0.027*\"wins\" + 0.027*\"ballot\" + 0.024*\"last\" + 0.020*\"gets\" + 0.019*\"harris\" + 0.019*\"please\" + 0.018*\"pitch\" + 0.018*\"equality\" + 0.018*\"kamala\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.054*\"vote\" + 0.046*\"support\" + 0.035*\"trump\" + 0.031*\"election\" + 0.026*\"make\" + 0.024*\"biden\" + 0.023*\"early\" + 0.023*\"call\" + 0.023*\"police\" + 0.023*\"continue\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.074*\"trump\" + 0.037*\"vote\" + 0.035*\"voting\" + 0.019*\"biden\" + 0.014*\"think\" + 0.011*\"breaking\" + 0.011*\"tomorrow\" + 0.011*\"give\" + 0.011*\"victory\" + 0.011*\"democrats\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.078*\"trump\" + 0.048*\"biden\" + 0.030*\"vote\" + 0.028*\"want\" + 0.027*\"voted\" + 0.016*\"anything\" + 0.015*\"polls\" + 0.014*\"mean\" + 0.014*\"supporters\" + 0.014*\"country\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.067*\"america\" + 0.043*\"trump\" + 0.030*\"heard\" + 0.026*\"believe\" + 0.021*\"clear\" + 0.021*\"vote\" + 0.020*\"votes\" + 0.017*\"message\" + 0.017*\"going\" + 0.016*\"never\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.037*\"biden\" + 0.032*\"trump\" + 0.028*\"ever\" + 0.020*\"president\" + 0.018*\"american\" + 0.017*\"people\" + 0.016*\"vote\" + 0.015*\"remember\" + 0.015*\"never\" + 0.015*\"would\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.047*\"opportunity\" + 0.047*\"economy\" + 0.044*\"promise\" + 0.044*\"education\" + 0.043*\"hispanic\" + 0.043*\"americans\" + 0.043*\"thriving\" + 0.043*\"provide\" + 0.043*\"fuel\" + 0.018*\"taxes\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.054*\"para\" + 0.053*\"trump\" + 0.048*\"spera\" + 0.048*\"econom\" + 0.047*\"impulsar\" + 0.047*\"proveer\" + 0.047*\"promesa\" + 0.047*\"oportunida\" + 0.042*\"answer\" + 0.025*\"trying\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.065*\"trump\" + 0.032*\"back\" + 0.027*\"going\" + 0.025*\"covid\" + 0.025*\"build\" + 0.021*\"spreader\" + 0.021*\"people\" + 0.020*\"time\" + 0.017*\"biden\" + 0.016*\"together\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.039*\"trump\" + 0.028*\"politician\" + 0.026*\"biden\" + 0.021*\"always\" + 0.021*\"fuck\" + 0.020*\"work\" + 0.019*\"like\" + 0.016*\"promises\" + 0.015*\"sound\" + 0.014*\"father\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.052*\"stop\" + 0.038*\"radical\" + 0.037*\"order\" + 0.036*\"signed\" + 0.036*\"commission\" + 0.036*\"students\" + 0.036*\"indoctrination\" + 0.036*\"establish\" + 0.025*\"trump\" + 0.024*\"biden\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.053*\"trump\" + 0.042*\"like\" + 0.037*\"america\" + 0.034*\"biden\" + 0.030*\"president\" + 0.027*\"actually\" + 0.025*\"choose\" + 0.022*\"american\" + 0.022*\"need\" + 0.021*\"better\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.101*\"trump\" + 0.062*\"biden\" + 0.046*\"donald\" + 0.022*\"election\" + 0.015*\"said\" + 0.015*\"know\" + 0.015*\"would\" + 0.014*\"fire\" + 0.014*\"fauci\" + 0.014*\"plan\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.110*\"crowd\" + 0.108*\"city\" + 0.106*\"michigan\" + 0.105*\"traverse\" + 0.102*\"landed\" + 0.041*\"trump\" + 0.017*\"biden\" + 0.012*\"america\" + 0.012*\"prison\" + 0.010*\"turn\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.071*\"biden\" + 0.048*\"trump\" + 0.021*\"americans\" + 0.015*\"best\" + 0.014*\"look\" + 0.014*\"think\" + 0.014*\"working\" + 0.013*\"votes\" + 0.013*\"true\" + 0.013*\"helps\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.068*\"trump\" + 0.057*\"biden\" + 0.024*\"president\" + 0.022*\"campaign\" + 0.020*\"said\" + 0.016*\"black\" + 0.016*\"need\" + 0.015*\"americans\" + 0.015*\"economy\" + 0.014*\"hispanic\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.041*\"biden\" + 0.034*\"support\" + 0.029*\"trump\" + 0.027*\"pennsylvania\" + 0.022*\"vote\" + 0.021*\"people\" + 0.020*\"scranton\" + 0.019*\"president\" + 0.018*\"continue\" + 0.017*\"right\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.066*\"trump\" + 0.032*\"vote\" + 0.030*\"biden\" + 0.026*\"james\" + 0.025*\"lebron\" + 0.025*\"american\" + 0.024*\"gave\" + 0.024*\"access\" + 0.023*\"children\" + 0.022*\"make\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.076*\"trump\" + 0.063*\"president\" + 0.040*\"america\" + 0.038*\"people\" + 0.029*\"years\" + 0.019*\"biden\" + 0.015*\"election\" + 0.014*\"ballots\" + 0.013*\"like\" + 0.012*\"mail\"\n",
      "\n",
      "Score: 0.010000149719417095\t \n",
      "Topic: 0.115*\"michigan\" + 0.106*\"city\" + 0.105*\"traverse\" + 0.075*\"crowd\" + 0.073*\"landed\" + 0.032*\"rally\" + 0.030*\"force\" + 0.029*\"beautiful\" + 0.028*\"sunset\" + 0.027*\"stunningly\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
